{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 325,
   "outputs": [
    {
     "data": {
      "text/plain": "30"
     },
     "execution_count": 325,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import math\n",
    "import datetime as dt\n",
    "import statistics\n",
    "import random\n",
    "import heapq\n",
    "from scipy import optimize\n",
    "from scipy.stats import norm\n",
    "from scipy.stats import pearsonr\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn import metrics\n",
    "from scipy.ndimage import uniform_filter1d\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import changefinder\n",
    "from loguru import logger\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from itertools import combinations\n",
    "\n",
    "logger.remove()\n",
    "logger.add(\"C:\\\\Users\\\\52497\\\\Dropbox\\\\VIS\\\\Insight\\\\Code\\\\NBA.log\", format=\"{time}{message}\",rotation=\"500 MB\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_list = ['carSales1', 'carSales2', 'Emission', 'Census', 'USBirth', 'AUSWeather', 'NBA', 'COVID-19']\n",
    "data_index = 2\n",
    "dataset = dataset_list[data_index]\n",
    "folder_path = \"C:\\\\Users\\\\52497\\\\Dropbox\\\\VIS\\\\Insight\\\\\"\n",
    "data_folder = folder_path + \"Data\\\\\"\n",
    "result_folder = folder_path + \"res\\\\\"\n",
    "\n",
    "raw_data_path = os.path.join(data_folder, '{}.csv'.format(dataset))\n",
    "insight_data_path = os.path.join(result_folder, 'insights_{}.csv'.format(dataset))\n",
    "\n",
    "data = pd.read_csv(raw_data_path)\n",
    "data = data.loc[(data!=0).any(axis=1)].dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def subspace_ordering(feature_measures, df):\n",
    "    feature_unique_value_matrix = dict(zip(feature_measures,\n",
    "                                       [df[feature].nunique() for feature in feature_measures]))\n",
    "    sorted_feature_dict = {k: v for k, v in sorted(feature_unique_value_matrix.items(), key=lambda item: item[1], reverse=False)}\n",
    "    return list(sorted_feature_dict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "       Year State             Producer Type Energy Source  CO2 (kt)  SO2 kt  \\\n0      1990    AK          Commercial Cogen          Coal   821.929  13.191   \n1      1991    AK          Commercial Cogen          Coal   848.745   8.359   \n2      1992    AK          Commercial Cogen          Coal   860.878   8.469   \n3      1993    AK          Commercial Cogen          Coal   858.244   8.393   \n4      1994    AK          Commercial Cogen          Coal   866.661   8.880   \n...     ...   ...                       ...           ...       ...     ...   \n41151  2014    WY  Utility Sector Non-Cogen   Natural Gas    27.760   0.000   \n41152  2015    WY  Utility Sector Non-Cogen   Natural Gas     0.395   0.000   \n41153  2005    WY  Utility Sector Non-Cogen     Petroleum     0.000   0.002   \n41154  2006    WY  Utility Sector Non-Cogen     Petroleum     0.317   0.002   \n41155  2007    WY  Utility Sector Non-Cogen     Petroleum     0.275   0.002   \n\n       NOx kt  \n0       3.009  \n1       3.146  \n2       3.195  \n3       3.208  \n4       3.238  \n...       ...  \n41151   0.056  \n41152   0.000  \n41153   0.023  \n41154   0.027  \n41155   0.025  \n\n[41156 rows x 7 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Year</th>\n      <th>State</th>\n      <th>Producer Type</th>\n      <th>Energy Source</th>\n      <th>CO2 (kt)</th>\n      <th>SO2 kt</th>\n      <th>NOx kt</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1990</td>\n      <td>AK</td>\n      <td>Commercial Cogen</td>\n      <td>Coal</td>\n      <td>821.929</td>\n      <td>13.191</td>\n      <td>3.009</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1991</td>\n      <td>AK</td>\n      <td>Commercial Cogen</td>\n      <td>Coal</td>\n      <td>848.745</td>\n      <td>8.359</td>\n      <td>3.146</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1992</td>\n      <td>AK</td>\n      <td>Commercial Cogen</td>\n      <td>Coal</td>\n      <td>860.878</td>\n      <td>8.469</td>\n      <td>3.195</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1993</td>\n      <td>AK</td>\n      <td>Commercial Cogen</td>\n      <td>Coal</td>\n      <td>858.244</td>\n      <td>8.393</td>\n      <td>3.208</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1994</td>\n      <td>AK</td>\n      <td>Commercial Cogen</td>\n      <td>Coal</td>\n      <td>866.661</td>\n      <td>8.880</td>\n      <td>3.238</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>41151</th>\n      <td>2014</td>\n      <td>WY</td>\n      <td>Utility Sector Non-Cogen</td>\n      <td>Natural Gas</td>\n      <td>27.760</td>\n      <td>0.000</td>\n      <td>0.056</td>\n    </tr>\n    <tr>\n      <th>41152</th>\n      <td>2015</td>\n      <td>WY</td>\n      <td>Utility Sector Non-Cogen</td>\n      <td>Natural Gas</td>\n      <td>0.395</td>\n      <td>0.000</td>\n      <td>0.000</td>\n    </tr>\n    <tr>\n      <th>41153</th>\n      <td>2005</td>\n      <td>WY</td>\n      <td>Utility Sector Non-Cogen</td>\n      <td>Petroleum</td>\n      <td>0.000</td>\n      <td>0.002</td>\n      <td>0.023</td>\n    </tr>\n    <tr>\n      <th>41154</th>\n      <td>2006</td>\n      <td>WY</td>\n      <td>Utility Sector Non-Cogen</td>\n      <td>Petroleum</td>\n      <td>0.317</td>\n      <td>0.002</td>\n      <td>0.027</td>\n    </tr>\n    <tr>\n      <th>41155</th>\n      <td>2007</td>\n      <td>WY</td>\n      <td>Utility Sector Non-Cogen</td>\n      <td>Petroleum</td>\n      <td>0.275</td>\n      <td>0.002</td>\n      <td>0.025</td>\n    </tr>\n  </tbody>\n</table>\n<p>41156 rows Ã— 7 columns</p>\n</div>"
     },
     "execution_count": 328,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [],
   "source": [
    "def point_power_law(phi):\n",
    "    \"\"\"\n",
    "    :param: phi\n",
    "    :return: [breakdown_value, observation_value, predict_value]\n",
    "    \"\"\"\n",
    "    ordered_phi = {k: v for k, v in sorted(phi.items(), key=lambda item: item[1], reverse=True)}\n",
    "    keys = list(ordered_phi.keys())\n",
    "    values = list(ordered_phi.values())\n",
    "    # print(values)\n",
    "    ydata = values[1::]\n",
    "    ydata = [i for i in ydata if i != 0]\n",
    "    if len(values) < 4:\n",
    "        return None\n",
    "    xdata = range(2, len(ydata) + 2)\n",
    "    logx = np.log10(xdata)\n",
    "    logy = np.log10(ydata)\n",
    "    pinit = [1.0]\n",
    "\n",
    "    fitfunc = lambda p, x: p[0] - 0.7 * x\n",
    "    errfunc = lambda p, x, y: (y - fitfunc(p, x))\n",
    "    powerLawFunc = lambda amp, index, x: amp * (x ** index)\n",
    "    try:\n",
    "        out = optimize.leastsq(errfunc, pinit, args=(logx, logy), full_output=1)\n",
    "\n",
    "        pfinal = out[0]\n",
    "        covar = out[1]\n",
    "\n",
    "        index = - 0.7\n",
    "        amp = 10.0 ** pfinal[0]\n",
    "        # indexErr = np.sqrt(covar[1][1])\n",
    "        # ampErr = np.sqrt(covar[0][0]) * amp\n",
    "\n",
    "\n",
    "        # predict_errs = ([1] + ydata) - powerLawFunc(amp, index, range(1, len(ydata) + 2))\n",
    "        # print(predict_errs)\n",
    "        # mu, std = norm.fit(predict_errs)\n",
    "        # plt.hist(predict_errs, density=True, alpha=0.6, color='g')\n",
    "        # plt_xmin, plt_xmax = plt.xlim()\n",
    "        # plt_x = np.linspace(plt_xmin, plt_xmax, 100)\n",
    "        # p = norm.pdf(plt_x, mu, std)\n",
    "        # plt.plot(plt_x, p, 'k', linewidth=2)\n",
    "        # title = \"Fit results: mu = %.2f,  std = %.2f\" % (mu, std)\n",
    "        # plt.title(title)\n",
    "        # plt.show()\n",
    "        residuals = []\n",
    "        for idx, val in enumerate(xdata):\n",
    "            residuals.append(ydata[idx] - powerLawFunc(amp, index, xdata[idx]))\n",
    "        mean,std=norm.fit(residuals)\n",
    "        sig = norm.cdf(values[0] - powerLawFunc(amp, index, 1), mean, std)\n",
    "        # plt.bar(range(1,len(keys)+1), values)\n",
    "        # plt.title(\"SIG: \" + str(sig) + \"val: \" + str(values[0] - powerLawFunc(amp, index, 1)))\n",
    "        # predict_res = [powerLawFunc(amp, index, i) for i in range(1,len(keys)+1)]\n",
    "        # plt.plot(range(1,len(keys)+1), predict_res, color='red')\n",
    "        # plt.show()\n",
    "        # print(predict_err, sig)\n",
    "        #  [breakdown_value, sig]\n",
    "        return [keys[0], sig]\n",
    "\n",
    "    except:\n",
    "        return [keys[0], -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_top1_insight(grouped_df, breakdown, measure):\n",
    "    global sum_impact\n",
    "    breakdown_measure_dict = dict(zip(list(grouped_df[breakdown]),list(grouped_df[measure])))\n",
    "    res = point_power_law(breakdown_measure_dict)\n",
    "\n",
    "    if not res: return None\n",
    "\n",
    "    result_dict = dict(zip(\n",
    "    ['breakdown','breakdown_value', 'sig'],\n",
    "    [breakdown] + res ))\n",
    "    return result_dict\n",
    "\n",
    "def get_subspace_df(subspace_condition_dict, df):\n",
    "    condition = pd.Series(True, index=df.index)\n",
    "    for feature in subspace_condition_dict:\n",
    "        if subspace_condition_dict[feature] == '*':\n",
    "            continue\n",
    "        condition = condition & (df[feature] == subspace_condition_dict[feature])\n",
    "    return df[condition]\n",
    "\n",
    "def calc_top1_insights(subspace_condition_dict, breakdown, measure, subspace_df, impact):\n",
    "    global sum_impact\n",
    "    if subspace_df.shape[0] == 0:\n",
    "        return None\n",
    "    grouped_df = subspace_df.groupby(breakdown, as_index=False).sum()\n",
    "    calc_dict = calc_top1_insight(grouped_df, breakdown, measure)\n",
    "    if not calc_dict: return None\n",
    "    res_dict = dict(zip(['impact', 'insight', 'insight_type', 'score', 'measure'],\n",
    "                        [impact, 'top1', 'point', calc_dict.get('sig')*impact, measure]))\n",
    "    top1_insight = dict(subspace_condition_dict, **calc_dict, **res_dict)\n",
    "    return top1_insight\n",
    "\n",
    "def calc_trend_insights(subspace_condition_dict, breakdown, measure, subspace_df, impact):\n",
    "    global time_col\n",
    "    global sum_impact\n",
    "    if subspace_df.shape[0] == 0 or breakdown != time_col or subspace_condition_dict[time_col] != '*':\n",
    "        return None\n",
    "    grouped_df = subspace_df.groupby(breakdown, as_index=False).sum()\n",
    "    #  Calculate trend insight\n",
    "    try:\n",
    "        x = grouped_df[breakdown].map(dt.datetime.toordinal).values #\n",
    "    except:\n",
    "        x = grouped_df[breakdown].values #\n",
    "    y = grouped_df[measure].values #\n",
    "    x = x.reshape(-1, 1) #\n",
    "    y = y.reshape(-1, 1) #\n",
    "    if x.shape[0] < 2: return None\n",
    "    reg = LinearRegression().fit(x, y) #\n",
    "    slope = reg.coef_[0][0] #\n",
    "    r2_score = reg.score(x, y)\n",
    "    # sig = r2_score * norm.cdf(abs(slope), 0.2, 10000)\n",
    "    sig = r2_score\n",
    "    # grouped_df.plot(x=breakdown, y=measure)\n",
    "    # plt.plot(grouped_df[breakdown], reg.predict(x))\n",
    "    # plt.title(\"Sig Score is: {}, subspace condition: {}\".format(sig, subspace_condition_dict))\n",
    "    # plt.show()\n",
    "    # print(subspace_condition_dict, \"Sig: \", sig, \"Impact: \", impact)\n",
    "    result_dict = dict(zip(['breakdown','breakdown_value', 'measure', 'sig', 'impact', 'insight', 'insight_type',\n",
    "                            'score'],\n",
    "                           [breakdown, -1] + [measure] + [sig, impact] + ['trend', 'shape', sig*impact]))\n",
    "    trend_insight = dict(subspace_condition_dict, **result_dict)\n",
    "    return trend_insight\n",
    "\n",
    "def calc_change_point_abs(y_MA, MA_size):\n",
    "    y_change = []\n",
    "    for index in range(len(y_MA)):\n",
    "        if index - MA_size < 0 or index + MA_size > len(y_MA) - 1:\n",
    "            y_change.append(0)\n",
    "        else:\n",
    "            y_change.append(abs(y_MA[index - round(MA_size / 2.0)] - y_MA[index + round(MA_size / 2.0)]))\n",
    "    return y_change\n",
    "\n",
    "def calc_change_point_sigma(y_MA, y_MA_square, MA_size):\n",
    "    y_sigma = []\n",
    "    for i in range(len(y_MA)):\n",
    "        if i - MA_size < 0 or i + MA_size > len(y_MA) - 1:\n",
    "            y_sigma.append(1)\n",
    "        else:\n",
    "            left_part = (y_MA_square[i-round(MA_size/2.0)] + y_MA_square[i+round(MA_size/2.0)]) / (2.0 * MA_size)\n",
    "            right_part = ((y_MA[i-round(MA_size/2.0)] + y_MA[i+round(MA_size/2.0)]) / (2.0 * MA_size)) ** 2\n",
    "            y_sigma.append(math.sqrt((left_part - right_part) / MA_size))\n",
    "        # if index == 0 or index == len(y) - 1:\n",
    "        #     y_sigma.append(0)\n",
    "        # elif 1 <= index < len(y) - 2:\n",
    "        #     res = np.sqrt((y_MA_square[index-1] + y_MA_square[index+2])/2 - np.square(y_MA[index-1] + y_MA[index+2]) / 4)\n",
    "        #     y_sigma.append(res)\n",
    "        # elif index == len(y) - 2:\n",
    "        #     res = np.sqrt((y_MA_square[index-1] + y[index+1]**2)/2 - np.square(y_MA[index-1] + y[index+1]) / 4)\n",
    "        #     y_sigma.append(res)\n",
    "    return y_sigma\n",
    "\n",
    "def calc_change_point_insights_mean_value(subspace_condition_dict, breakdown, measure, subspace_df, impact):\n",
    "    global sum_impact\n",
    "    MA_size = 5\n",
    "    if subspace_df.shape[0] == 0 or breakdown != time_col or subspace_condition_dict[time_col] != '*':\n",
    "        return None\n",
    "    grouped_df = subspace_df.groupby(breakdown, as_index=False).sum()\n",
    "    x = grouped_df[breakdown].values\n",
    "    y = grouped_df[measure].values\n",
    "    if len(y) < 20:\n",
    "        return None\n",
    "    y_MA = uniform_filter1d(y, size=MA_size, mode='nearest')\n",
    "    y_MA_square = uniform_filter1d(np.square(y), size=MA_size, mode='nearest')\n",
    "    y_change = calc_change_point_abs(y_MA, MA_size)\n",
    "    y_sigma = calc_change_point_sigma(y_MA=y_MA, y_MA_square=y_MA_square, MA_size=MA_size)\n",
    "    k_mean = [y_change[i] / y_sigma[i] for i in range(len(y))]\n",
    "    k_mean_max = max(k_mean)\n",
    "    k_mean_max_index = k_mean.index(k_mean_max)\n",
    "    sig = norm.cdf(k_mean_max * np.square(MA_size))\n",
    "    breakdown_value = x[k_mean_max_index]\n",
    "    # print(\"The significance is {}, with value = {}\".format(sig, breakdown_value))\n",
    "    result_dict = dict(zip(['breakdown','breakdown_value', 'measure', 'sig', 'impact', 'insight', 'insight_type',\n",
    "                            'score'],\n",
    "                           [breakdown, breakdown_value] + [measure] +[sig, impact]\n",
    "                           + ['change point', 'shape', sig*impact]))\n",
    "    change_point_insight = dict(subspace_condition_dict, **result_dict)\n",
    "    # print(k_mean)\n",
    "    # print(k_mean.index(k_mean_max))\n",
    "    # if sig > 0.8:\n",
    "    #     print(\"-----------------------------\")\n",
    "    #     plt.plot(x,y)\n",
    "    #     plt.plot([breakdown_value], [y[k_mean.index(k_mean_max)]],color='red', marker='o', markersize=3)\n",
    "    #     plt.show()\n",
    "    if not sig:\n",
    "        return None\n",
    "    return change_point_insight\n",
    "\n",
    "def calc_change_point_insights_slope(subspace_condition_dict, breakdown, measure, subspace_df, impact):\n",
    "    global sum_impact\n",
    "    MA_size = 2\n",
    "    if subspace_df.shape[0] == 0 or breakdown != time_col or subspace_condition_dict[time_col] != '*':\n",
    "        return None\n",
    "    grouped_df = subspace_df.groupby(breakdown, as_index=False).sum()\n",
    "    x = grouped_df[breakdown].values\n",
    "    y = grouped_df[measure].values\n",
    "    if len(y) < 4:\n",
    "        return None\n",
    "    # Stores the slope between x_i-1 and x_i on slope_list[i]\n",
    "    modified_y = [(y[i]-min(y))/(max(y)-min(y)) for i in range(0, len(y))]\n",
    "    slope_list = [(modified_y[i]-modified_y[i-1]) if i != 0 else 0 for i in range(0, len(y))]\n",
    "    slope_diff_abs = [abs(slope_list[i]-slope_list[i+1]) if i != 0 else 0 for i in range(0, len(y)-1)] + [0]\n",
    "    # sigma = [0.0] + [math.sqrt((y[i-1]**2 + y[i]**2 + y[i+1]**2)/6 - (y[i-1]+y[i]+y[i+1])**2 / (4*9))/math.sqrt(3)\n",
    "    #          for i in range(1, len(y)-1)] + [0.0]\n",
    "    # k_slope = [slope_diff_abs[i]/sigma[i] if sigma[i]!=0 else 0 for i in range(0, len(y))]\n",
    "    k_slope = [slope_diff_abs[i] for i in range(0, len(y))]\n",
    "    k_slope_max = max(k_slope)\n",
    "    sig = norm.cdf(k_slope_max)\n",
    "    breakdown_value = x[k_slope.index(k_slope_max)]\n",
    "    # print(\"The significance is {}, with value = {}\".format(sig, breakdown_value))\n",
    "    result_dict = dict(zip(['breakdown','breakdown_value', 'measure', 'sig', 'impact', 'insight', 'insight_type',\n",
    "                            'score'],\n",
    "                           [breakdown, breakdown_value] + [measure] + [sig, impact] + ['change point', 'shape', sig*impact]))\n",
    "    change_point_insight = dict(subspace_condition_dict, **result_dict)\n",
    "    # plt.title('Sig: '+ str(sig)+\" slope left: \"+ str(slope_list[k_slope.index(k_slope_max)]/max(y))+\" slope right: \"+ str(slope_list[k_slope.index(k_slope_max)+1]/max(y)))\n",
    "    # plt.plot(x,y)\n",
    "    # plt.plot([breakdown_value], [y[k_slope.index(k_slope_max)]],color='red', marker='o', markersize=3)\n",
    "    # plt.show()\n",
    "    if not sig:\n",
    "        return None\n",
    "    return change_point_insight\n",
    "\n",
    "def calc_change_point_insights_changefinder(subspace_condition_dict, breakdown, measure, subspace_df, impact):\n",
    "    global sum_impact\n",
    "    if subspace_df.shape[0] == 0 or breakdown != time_col or subspace_condition_dict[time_col] != '*':\n",
    "        return None\n",
    "    grouped_df = subspace_df.groupby(breakdown, as_index=False).sum()\n",
    "    x = grouped_df[breakdown].values\n",
    "    y = grouped_df[measure].values\n",
    "    if len(y) < 4:\n",
    "        return None\n",
    "    #CHANGEFINDER PACKAGE\n",
    "    # f, (ax1, ax2) = plt.subplots(2, 1)\n",
    "    # f.subplots_adjust(hspace=0.4)\n",
    "    # ax1.plot(y)\n",
    "    # ax1.set_title(\"data point\")\n",
    "    #Initiate changefinder function\n",
    "    cf = changefinder.ChangeFinder()\n",
    "    scores = [cf.update(int(p)) for p in y]\n",
    "    # ax2.plot(scores)\n",
    "    # ax2.set_title(\"anomaly score\")\n",
    "    # plt.show()\n",
    "    score_max = max(scores)\n",
    "    sig = norm.cdf(score_max/15.0)\n",
    "    breakdown_value = x[scores.index(score_max)]\n",
    "    # print(\"The significance is {}, with value = {}\".format(sig, breakdown_value))\n",
    "    result_dict = dict(zip(['breakdown','breakdown_value', 'measure', 'sig', 'impact', 'insight', 'insight_type',\n",
    "                            'score'],\n",
    "                           [breakdown, breakdown_value] + [measure] + [sig, impact] + ['change point', 'shape', sig*impact]))\n",
    "    change_point_insight = dict(subspace_condition_dict, **result_dict)\n",
    "    if not sig:\n",
    "        return None\n",
    "    return change_point_insight\n",
    "\n",
    "def calc_outlier_insights(subspace_condition_dict, breakdown, measure, subspace_df, impact):\n",
    "    global sum_impact\n",
    "    if subspace_df.shape[0] == 0 or breakdown != time_col or subspace_condition_dict[time_col] != '*':\n",
    "        return None\n",
    "    grouped_df = subspace_df.groupby(breakdown, as_index=False).sum()\n",
    "    x = grouped_df[breakdown].values\n",
    "    y = grouped_df[measure].values\n",
    "    if len(y) < 4:\n",
    "        return None\n",
    "    mean = np.mean(y)\n",
    "    std = np.std(y)\n",
    "    z_score = (y - mean) / std\n",
    "    z_score_max_index = np.argmax(z_score)\n",
    "    z_score_max = z_score[z_score_max_index]\n",
    "    sig = norm.cdf(z_score_max)\n",
    "    breakdown_value = x[z_score_max_index]\n",
    "    if math.isnan(sig) :\n",
    "        return None\n",
    "    # print(\"The significance is {}, with value = {}, kmeans is {}\".format(sig, breakdown_value, z_score_max))\n",
    "    result_dict = dict(zip(['breakdown','breakdown_value', 'measure', 'sig', 'impact', 'insight', 'insight_type',\n",
    "                            'score'],\n",
    "                           [breakdown, breakdown_value] + [measure] + [sig, impact] + ['outlier', 'shape', sig*impact]))\n",
    "    outlier_insight = dict(subspace_condition_dict, **result_dict)\n",
    "    # print(\"-----------------------------\")\n",
    "    # plt.plot(x,y)\n",
    "    # plt.plot([breakdown_value], [y[z_score_max_index]],color='red', marker='o', markersize=3)\n",
    "    # plt.show()\n",
    "    return outlier_insight\n",
    "\n",
    "def calc_attribution_insights(subspace_condition_dict, breakdown, measure, subspace_df, impact):\n",
    "    global sum_impact\n",
    "    if subspace_df.shape[0] == 0:\n",
    "        return None\n",
    "    grouped_df = subspace_df.groupby(breakdown, as_index=False).sum()\n",
    "    x = grouped_df[breakdown].values\n",
    "    y = grouped_df[measure].values\n",
    "    if np.sum(y) == 0:\n",
    "        return None\n",
    "    portion = np.max(y)/np.sum(y)\n",
    "\n",
    "\n",
    "    sig = norm.cdf(portion, 0.5, 0.3)\n",
    "    breakdown_index = np.argmax(y)\n",
    "    result_dict = dict(zip(['breakdown','breakdown_value', 'measure', 'sig', 'impact', 'insight', 'insight_type',\n",
    "                            'score'],\n",
    "                           [breakdown, x[breakdown_index]] + [measure] + [sig, impact] + ['attribution', 'point', sig*impact]))\n",
    "    attribution_insight = dict(subspace_condition_dict, **result_dict)\n",
    "    return attribution_insight\n",
    "\n",
    "def calc_evenness_insights(subspace_condition_dict, breakdown, measure, subspace_df, impact):\n",
    "    global sum_impact\n",
    "    if subspace_df.shape[0] == 0 or breakdown != time_col or subspace_condition_dict[time_col] != '*':\n",
    "        return None\n",
    "    grouped_df = subspace_df.groupby(breakdown, as_index=False).sum()\n",
    "    x = grouped_df[breakdown].values\n",
    "    y = grouped_df[measure].values\n",
    "    if len(y) == 1: return None\n",
    "    std = np.std(y)\n",
    "    sig = 2 * (1 - norm.cdf(std, 0, 3))\n",
    "    result_dict = dict(zip(['breakdown','breakdown_value', 'measure', 'sig', 'impact', 'insight', 'insight_type',\n",
    "                            'score'],\n",
    "                           [breakdown, -1] + [sig, impact] + [measure] + ['evenness', 'point', sig*impact]))\n",
    "    evenness_insight = dict(subspace_condition_dict, **result_dict)\n",
    "    return evenness_insight\n",
    "\n",
    "def calc_correlation_insights(subspace_condition_dict, breakdown, measure, subspace_df, impact,\n",
    "                              corr_subspace_condition_dict, corr_subspace_df):\n",
    "    global time_col\n",
    "    global sum_impact\n",
    "    if subspace_df.shape[0] == 0 or breakdown != time_col or subspace_condition_dict[time_col] != '*':\n",
    "        return None\n",
    "    if corr_subspace_df.shape[0] == 0 or corr_subspace_condition_dict[time_col] != '*':\n",
    "        return None\n",
    "    grouped_df = subspace_df.groupby(breakdown, as_index=False).sum()\n",
    "    corr_grouped_df = corr_subspace_df.groupby(breakdown, as_index=False).sum()\n",
    "    #  Calculate trend insight\n",
    "    x = grouped_df[breakdown].values\n",
    "    if len(x) != len(corr_grouped_df[breakdown].values) \\\n",
    "            or not (x == corr_grouped_df[breakdown].values).all()\\\n",
    "            or len(x) < 2:\n",
    "        logger.info(\"data not matched\")\n",
    "        return None\n",
    "    y1 = grouped_df[measure].values\n",
    "    y2 = corr_grouped_df[measure].values\n",
    "    sig, _ = pearsonr(y1, y2)\n",
    "    return sig\n",
    "\n",
    "def calc_cross_measure_correlation_insights(subspace_condition_dict, breakdown, measure, subspace_df, impact):\n",
    "    global sum_impact\n",
    "    if subspace_df.shape[0] == 0:\n",
    "        return None\n",
    "    grouped_df = subspace_df.groupby(breakdown, as_index=False).sum()\n",
    "    measure_val_x = grouped_df[measures[0]].values.reshape(-1,1)\n",
    "    measure_val_y = grouped_df[measures[1]].values.reshape(-1,1)\n",
    "    if measure_val_x.shape[0] < 10: return None\n",
    "    reg = LinearRegression().fit(measure_val_x, measure_val_y) #\n",
    "    slope = reg.coef_[0][0] #\n",
    "    r2_score = reg.score(measure_val_x, measure_val_y)\n",
    "    sig = r2_score\n",
    "    # if sig > 0.5:\n",
    "    #     plt.title('Breakdown: ' + breakdown + \"Sig: \" + str(sig))\n",
    "    #     plt.scatter(measure_val_x, measure_val_y)\n",
    "    #     measure_y_pred = reg.predict(measure_val_x)\n",
    "    #     plt.plot(measure_val_x, measure_y_pred)\n",
    "    #     plt.show()\n",
    "    # else:\n",
    "    #     print(\"Sig is: \", sig)\n",
    "    result_dict = dict(zip(['breakdown','breakdown_value', 'measure', 'sig', 'impact', 'insight', 'insight_type',\n",
    "                            'score'],\n",
    "                           [breakdown, -1] + [';'.join(measures)] + [sig, impact] +  ['cross measure correlation', 'compound', sig*impact]))\n",
    "    cross_insight = dict(subspace_condition_dict, **result_dict)\n",
    "    return cross_insight\n",
    "\n",
    "\n",
    "from itertools import cycle\n",
    "from sklearn.cluster import MeanShift, estimate_bandwidth\n",
    "import collections\n",
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "def calc_clustering_insights(subspace_condition_dict, breakdown, measure, subspace_df, impact):\n",
    "    global sum_impact\n",
    "    if subspace_df.shape[0] == 0:\n",
    "        return None\n",
    "    grouped_df = subspace_df.groupby(breakdown, as_index=False).sum()\n",
    "    measure_val_x = np.array(grouped_df[measures[0]].values)\n",
    "    measure_val_y = np.array(grouped_df[measures[1]].values)\n",
    "    if measure_val_x.shape[0] < 30: return None\n",
    "    X = np.vstack((measure_val_x, measure_val_y)).T\n",
    "    X_scale = StandardScaler().fit_transform(X)\n",
    "\n",
    "    # Compute DBSCAN\n",
    "    db = DBSCAN(eps=0.3, min_samples=5).fit(X_scale)\n",
    "    core_samples_mask = np.zeros_like(db.labels_, dtype=bool)\n",
    "    core_samples_mask[db.core_sample_indices_] = True\n",
    "    labels = db.labels_\n",
    "\n",
    "    # Number of clusters in labels, ignoring noise if present.\n",
    "    n_clusters_ = len(set(labels)) - (1 if -1 in labels else 0)\n",
    "    n_noise_ = list(labels).count(-1)\n",
    "    counters = collections.Counter(labels)\n",
    "    max_group = max(counters, key=counters.get)\n",
    "    max_total_ratio = counters.get(max_group) * 1.0 / len(labels)\n",
    "\n",
    "    # #############################################################################\n",
    "    # Plot result\n",
    "    if n_clusters_ == 1:\n",
    "        score = 1\n",
    "    elif n_clusters_ == 0:\n",
    "        return None\n",
    "    else:\n",
    "        score = 0.5 * (metrics.silhouette_score(X, labels) + 1)\n",
    "    # if score > 0.5 and max_total_ratio > 0.8:\n",
    "    #     plt.figure(1)\n",
    "    #     plt.clf()\n",
    "    #\n",
    "    #     colors = cycle('bgrcmykbgrcmykbgrcmykbgrcmyk')\n",
    "    #     for k, col in zip(range(n_clusters_), colors):\n",
    "    #         my_members = labels == k\n",
    "    #         plt.plot(X[my_members, 0], X[my_members, 1], col + '.')\n",
    "    #             # plt.plot(cluster_center[0], cluster_center[1], 'o', markerfacecolor=col,\n",
    "    #             #          markeredgecolor='k', markersize=14)\n",
    "    #     plt.title('Estimated number of clusters: %d, Score: %0.2f, Size: %d' % (n_clusters_, score, len(measure_val_x)))\n",
    "    #     plt.show()\n",
    "    result_dict = dict(zip(['breakdown','breakdown_value', 'measure', 'sig', 'impact', 'insight', 'insight_type',\n",
    "                            'score'],\n",
    "                           [breakdown, -1] + [';'.join(measures)] + [score, impact]  + ['clustering', 'compound shape',\n",
    "                                                                (score + max_total_ratio)*impact*0.5]))\n",
    "    clustering_insight = dict(subspace_condition_dict, **result_dict)\n",
    "    return clustering_insight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "outputs": [],
   "source": [
    "def traverse_compound_insights(root, df, measure, feature_names, breakdown, top_num, output_array,\n",
    "                               subspace_condition_dict, calc_insights,\n",
    "                               subspace_df, impact):\n",
    "    if len(output_array) == top_num and impact < output_array[0][0] :\n",
    "        # print('Subspace pruned')\n",
    "        return output_array\n",
    "\n",
    "    subspaces = []\n",
    "    for col_index, val_index in enumerate(root):\n",
    "        if feature_names[col_index] == breakdown and val_index == '*':\n",
    "            available_bk_val = subspace_df[feature_names[col_index]].unique().tolist() + ['*']\n",
    "            subspaces += (generate_array(root, col_index, val) for val in available_bk_val)\n",
    "    # print('subspace: ', root)\n",
    "    # print('breakdown: ', breakdown)\n",
    "    # print('subspaces: ', subspaces)\n",
    "    subspace_comb = combinations(subspaces, 2)\n",
    "    for comb in list(subspace_comb):\n",
    "        sub0 = comb[0]\n",
    "        sub1 = comb[1]\n",
    "        sub_diff = []\n",
    "        for idx, _ in enumerate(sub0):\n",
    "            if sub0[idx] != sub1[idx]:\n",
    "                sub_diff = [sub0[idx], sub1[idx]]\n",
    "\n",
    "        corr_subspace_df1 = get_subspace_df(dict(zip(feature_names, sub1)), subspace_df)\n",
    "        corr_subspace_condition_dict1 = dict(zip(feature_names, sub1))\n",
    "        corr_impact1 = corr_subspace_df1[measure].count() / sum_impact\n",
    "\n",
    "        corr_subspace_df0 = get_subspace_df(dict(zip(feature_names, sub0)), subspace_df)\n",
    "        corr_subspace_condition_dict0 = dict(zip(feature_names, sub0))\n",
    "        corr_impact0 = corr_subspace_df0[measure].count() / sum_impact\n",
    "\n",
    "        sig = calc_insights(corr_subspace_condition_dict0, time_col, measure, corr_subspace_df0,\n",
    "                        impact,corr_subspace_condition_dict1,\n",
    "                        corr_subspace_df1)\n",
    "        if '*' in sub_diff:\n",
    "            impact = max(corr_impact0, corr_impact1)\n",
    "        else:\n",
    "            impact = corr_impact0 + corr_impact1\n",
    "        if sig:\n",
    "            result_dict = dict(zip(['breakdown','breakdown_value', 'measure', 'sig', 'impact', 'insight', 'insight_type',\n",
    "                'score'],\n",
    "               [breakdown, str(sub_diff[0]) + \";\" + str(sub_diff[1]), measure] + [sig, impact]\n",
    "                                   + ['correlation', 'compound shape', sig*impact]))\n",
    "            insight = dict(subspace_condition_dict, **result_dict)\n",
    "            logger.info(\"Has insight\")\n",
    "            if len(output_array) < top_num:\n",
    "                heapq.heappush(output_array, (insight.get('score'), random.random(), insight))\n",
    "                logger.info(\"Current Score LB: {}\".format(output_array[0][0]))\n",
    "            elif output_array[0][0] < insight.get('score'):\n",
    "                heapq.heapreplace(output_array, (insight.get('score'), random.random(), insight))\n",
    "                logger.info(\"Current Score LB: {}\".format(output_array[0][0]))\n",
    "\n",
    "    return output_array"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_array(arr, index, val):\n",
    "    new_arr = arr[:]\n",
    "    new_arr[index] = val\n",
    "    return new_arr\n",
    "\n",
    "def generate_process_node(feature_names, measure, output_array, df,\n",
    "                          calc_insights = lambda: \"Need to define\", breakdowns = [], top_num = 50):\n",
    "    def process_node(node, result_stack, breakdown):\n",
    "        nonlocal output_array\n",
    "        # subspace_condition_dict. e.g., ['Year', 'Brand', 'Category'] ['2007', '*', '*']\n",
    "        subspace_df = get_subspace_df(dict(zip(feature_names, node)), df)\n",
    "        if type(measure) == str:\n",
    "            impact = subspace_df[measure].count() / sum_impact\n",
    "        else:\n",
    "            impact = 0.5 * (subspace_df[measure[0]].count() / sum_impact0 + subspace_df[measure[1]].count() / sum_impact1)\n",
    "        subspace_condition_dict = dict(zip(feature_names, node))\n",
    "\n",
    "        if len(output_array) == top_num and output_array[0][0] > impact:\n",
    "            logger.info(\"Pruned %f, subspace: %s\"%(impact, subspace_condition_dict))\n",
    "            return False\n",
    "        logger.info(\"Impact not pruned %f, subspace: %s\"%(impact, subspace_condition_dict))\n",
    "        if calc_insights == calc_correlation_insights:\n",
    "            if breakdown != time_col:\n",
    "                output_array = traverse_compound_insights(root=node,\n",
    "                                                          df=df,\n",
    "                                                          feature_names=feature_names,\n",
    "                                                          breakdown=breakdown,\n",
    "                                                          measure=measure,\n",
    "                                                          top_num=top_num, output_array=output_array,\n",
    "                                                          subspace_condition_dict=subspace_condition_dict,\n",
    "                                                          calc_insights=calc_insights,\n",
    "                                                          subspace_df=subspace_df, impact=impact)\n",
    "        else:\n",
    "            insight = calc_insights(subspace_condition_dict=subspace_condition_dict,\n",
    "                         breakdown=breakdown, measure=measure, impact = impact, subspace_df=subspace_df)\n",
    "            if insight:\n",
    "                # output.append(insight)\n",
    "                if len(output_array) < top_num:\n",
    "                    modified_insight = dict(insight)\n",
    "                    heapq.heappush(output_array, (insight.get('score'), random.random(), modified_insight))\n",
    "                    logger.info(\"Current Score LB: {}\".format(output_array[0][0]))\n",
    "                elif output_array[0][0] < insight.get('score'):\n",
    "                    modified_insight = dict(insight)\n",
    "                    heapq.heapreplace(output_array, (insight.get('score'), random.random(), modified_insight))\n",
    "                    logger.info(\"Current Score LB: {}\".format(output_array[0][0]))\n",
    "            else:\n",
    "                # print(\"None Insight, subspace: %s, breakdown: %s\"%(subspace_condition_dict, breakdown))\n",
    "                return False\n",
    "        if node.count('*') == 1:\n",
    "            return False\n",
    "        return True\n",
    "    return process_node\n",
    "\n",
    "def BFS_tranverse_and_process(df, feature_names, process_node = lambda x: True, type=None):\n",
    "    feature_unique_value_matrix = [df[feature].unique().tolist() for feature in feature_names]\n",
    "    breakdown_list = feature_names.copy()\n",
    "\n",
    "    for breakdown in breakdown_list:\n",
    "        traverse_root = ['*' for i in feature_names] + [[i for (i, val) in enumerate(feature_names) if val == breakdown]]\n",
    "        result_stack = [traverse_root]\n",
    "\n",
    "        while len(result_stack) > 0:\n",
    "            *root, ban = result_stack.pop(0)\n",
    "            result = process_node(node=root, result_stack=[[*root, ban]]+ result_stack, breakdown=breakdown)\n",
    "            if not result:\n",
    "                continue\n",
    "            banned_index = ban[:]\n",
    "            subspace_df = get_subspace_df(dict(zip(feature_names, root)), df)\n",
    "            for col_index, val_index in enumerate(root):\n",
    "                if col_index in banned_index:\n",
    "                    continue\n",
    "                banned_index.append(col_index)\n",
    "                banned_index = list(set(banned_index))\n",
    "                available_features = subspace_df[feature_names[col_index]].unique().tolist()\n",
    "                if val_index == '*':\n",
    "                    result_stack += ([generate_array(root, col_index, val)+[banned_index[:]]  for val in available_features])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trend finished\n",
      "{'Producer Type': '*', 'Energy Source': '*', 'Year': '*', 'State': 'MS', 'breakdown': 'Year', 'breakdown_value': -1, 'measure': 'CO2 (kt)', 'sig': 0.561614612567644, 'impact': 0.014724463018757896, 'insight': 'trend', 'insight_type': 'shape', 'score': 0.008269473593546317}\n",
      "Top1 finished\n",
      "Change point finished\n",
      "Outlier finished\n",
      "Attribution finished\n",
      "Correlation finished\n",
      "Trend finished\n",
      "{'Producer Type': '*', 'Energy Source': '*', 'Year': '*', 'State': 'MS', 'breakdown': 'Year', 'breakdown_value': -1, 'measure': 'CO2 (kt)', 'sig': 0.561614612567644, 'impact': 0.014724463018757896, 'insight': 'trend', 'insight_type': 'shape', 'score': 0.008269473593546317}\n",
      "Top1 finished\n",
      "Change point finished\n",
      "Outlier finished\n",
      "Attribution finished\n",
      "Correlation finished\n",
      "Clustering finished\n",
      "Cross measure finished\n",
      "Time Elapsed: 189.3702461719513\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-330-28dac05bc291>:106: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  k_mean = [y_change[i] / y_sigma[i] for i in range(len(y))]\n",
      "<ipython-input-330-28dac05bc291>:205: RuntimeWarning: invalid value encountered in true_divide\n",
      "  z_score = (y - mean) / std\n",
      "C:\\Users\\52497\\.conda\\envs\\py38\\lib\\site-packages\\scipy\\stats\\stats.py:3845: PearsonRConstantInputWarning: An input array is constant; the correlation coefficent is not defined.\n",
      "  warnings.warn(PearsonRConstantInputWarning())\n",
      "C:\\Users\\52497\\.conda\\envs\\py38\\lib\\site-packages\\scipy\\stats\\stats.py:3845: PearsonRConstantInputWarning: An input array is constant; the correlation coefficent is not defined.\n",
      "  warnings.warn(PearsonRConstantInputWarning())\n",
      "C:\\Users\\52497\\.conda\\envs\\py38\\lib\\site-packages\\scipy\\stats\\_distn_infrastructure.py:1844: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  x = np.asarray((x - loc)/scale, dtype=dtyp)\n",
      "<ipython-input-330-28dac05bc291>:106: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  k_mean = [y_change[i] / y_sigma[i] for i in range(len(y))]\n",
      "<ipython-input-330-28dac05bc291>:205: RuntimeWarning: invalid value encountered in true_divide\n",
      "  z_score = (y - mean) / std\n",
      "C:\\Users\\52497\\.conda\\envs\\py38\\lib\\site-packages\\scipy\\stats\\stats.py:3845: PearsonRConstantInputWarning: An input array is constant; the correlation coefficent is not defined.\n",
      "  warnings.warn(PearsonRConstantInputWarning())\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "\n",
    "# CarSales1.csv\n",
    "# feature_names = ['Year', 'Brand', 'Category']\n",
    "# measures = ['Sales']\n",
    "# time_col = 'Year'\n",
    "# data[time_col] = data[time_col].apply(lambda year_string: year_string.split('/')[0])\n",
    "\n",
    "\n",
    "# carSales2.csv\n",
    "# feature_names = ['Year', 'Brand', 'Body', 'Engine Type', 'Model', 'Registration']\n",
    "# # measure = 'Price'\n",
    "# measure = 'Mileage'\n",
    "# time_col = 'Year'\n",
    "\n",
    "\n",
    "# Census.csv\n",
    "# feature_names = ['Birthday', 'Age Segment', 'Marital Status', 'Sex', 'Age Group']\n",
    "# measure = 'Count of persons'\n",
    "# time_col = 'Birthday'\n",
    "#\n",
    "# data[time_col] = data[time_col].apply(lambda year_string: year_string.split('/')[0])\n",
    "\n",
    "\n",
    "# Emission.csv\n",
    "feature_names = ['Year', 'State', 'Producer Type', 'Energy Source']\n",
    "measures = ['CO2 (kt)', 'SO2 kt']\n",
    "time_col = 'Year'\n",
    "\n",
    "# USBirth.csv\n",
    "# feature_names = ['Year', 'Age', 'Birth order']\n",
    "# measure = 'Value'\n",
    "# time_col = 'Year'\n",
    "\n",
    "# # AUSWeather.csv\n",
    "# feature_names = ['Date','Location','RainToday','RainTomorrow']\n",
    "# measures = ['MinTemp','MaxTemp']\n",
    "# measure = 'MinTemp'\n",
    "# time_col = 'Date'\n",
    "#\n",
    "# NBA.csv\n",
    "# feature_names = ['name','year','team_name','age','lg_name','pos_name']\n",
    "# measures = ['PTS','AST']\n",
    "# measure = 'PTS'\n",
    "# time_col = 'year'\n",
    "\n",
    "# COVID-19.csv\n",
    "# feature_names = ['date','county','state']\n",
    "# measures = ['cases', 'deaths']\n",
    "# measure = 'cases'\n",
    "# time_col = 'date'\n",
    "# data[time_col] = pd.to_datetime(data[time_col]).dt.date\n",
    "\n",
    "res = []\n",
    "for measure in measures:\n",
    "    sum_impact = data[measure].count()\n",
    "    feature_names = subspace_ordering(feature_measures=feature_names, df=data)\n",
    "    time_col_index = feature_names.index(time_col)\n",
    "\n",
    "    output = []\n",
    "    process_node_trend = generate_process_node(feature_names = feature_names,\n",
    "                                         output_array=output,\n",
    "                                         df = data,\n",
    "                                         measure = measure,\n",
    "                                         calc_insights=calc_trend_insights)\n",
    "\n",
    "    BFS_tranverse_and_process(data,feature_names, process_node=process_node_trend)\n",
    "    print(\"Trend finished\")\n",
    "    res += [out[-1] for out in output.copy()]\n",
    "    print(res[0])\n",
    "\n",
    "    output = []\n",
    "    process_node_top1 = generate_process_node(feature_names = feature_names,\n",
    "                                         output_array=output,\n",
    "                                         df = data,\n",
    "                                         measure = measure,\n",
    "                                         calc_insights=calc_top1_insights)\n",
    "\n",
    "    BFS_tranverse_and_process(data,feature_names, process_node=process_node_top1)\n",
    "    print(\"Top1 finished\")\n",
    "    res += [out[-1] for out in output.copy()]\n",
    "\n",
    "    output = []\n",
    "    #\n",
    "    process_node_change_point = generate_process_node(feature_names = feature_names,\n",
    "                                         output_array=output,\n",
    "                                         df = data,\n",
    "                                         measure = measure,\n",
    "                                         calc_insights=calc_change_point_insights_mean_value)\n",
    "\n",
    "    BFS_tranverse_and_process(data,feature_names, process_node=process_node_change_point)\n",
    "    print(\"Change point finished\")\n",
    "    res += [out[-1] for out in output.copy()]\n",
    "\n",
    "    output = []\n",
    "    process_node_outlier = generate_process_node(feature_names = feature_names,\n",
    "                                         output_array=output,\n",
    "                                         df = data,\n",
    "                                         measure = measure,\n",
    "                                         calc_insights=calc_outlier_insights)\n",
    "\n",
    "    BFS_tranverse_and_process(data,feature_names, process_node=process_node_outlier)\n",
    "    print(\"Outlier finished\")\n",
    "    res += [out[-1] for out in output.copy()]\n",
    "\n",
    "    output = []\n",
    "    process_node_attribution = generate_process_node(feature_names = feature_names,\n",
    "                                         output_array=output,\n",
    "                                         df = data,\n",
    "                                         measure = measure,\n",
    "                                         calc_insights=calc_attribution_insights)\n",
    "\n",
    "    BFS_tranverse_and_process(data,feature_names, process_node=process_node_attribution)\n",
    "    print(\"Attribution finished\")\n",
    "    res += [out[-1] for out in output.copy()]\n",
    "\n",
    "    output = []\n",
    "    process_node_correlation = generate_process_node(feature_names = feature_names,\n",
    "                                         output_array=output,\n",
    "                                         df = data,\n",
    "                                         measure = measure,\n",
    "                                         calc_insights=calc_correlation_insights)\n",
    "\n",
    "    BFS_tranverse_and_process(data,feature_names, process_node=process_node_correlation, type='time breakdown')\n",
    "    print(\"Correlation finished\")\n",
    "    res += [out[-1] for out in output.copy()]\n",
    "\n",
    "    output = []\n",
    "\n",
    "if len(measures) > 1:\n",
    "    sum_impact0 = data[measures[0]].count()\n",
    "    sum_impact1 = data[measures[1]].count()\n",
    "    feature_names = subspace_ordering(feature_measures=feature_names, df=data)\n",
    "    time_col_index = feature_names.index(time_col)\n",
    "    process_node_clustering = generate_process_node(feature_names = feature_names,\n",
    "                                         output_array=output,\n",
    "                                         df = data,\n",
    "                                         measure = measures,\n",
    "                                         calc_insights=calc_clustering_insights)\n",
    "\n",
    "    BFS_tranverse_and_process(data,feature_names, process_node=process_node_clustering)\n",
    "    print(\"Clustering finished\")\n",
    "    res += [out[-1] for out in output.copy()]\n",
    "\n",
    "    output = []\n",
    "    process_node_cross_measure = generate_process_node(feature_names = feature_names,\n",
    "                                         output_array=output,\n",
    "                                         df = data,\n",
    "                                         measure = measures,\n",
    "                                         calc_insights=calc_cross_measure_correlation_insights)\n",
    "\n",
    "    BFS_tranverse_and_process(data,feature_names, process_node=process_node_cross_measure)\n",
    "    print(\"Cross measure finished\")\n",
    "    res += [out[-1] for out in output.copy()]\n",
    "\n",
    "    output = []\n",
    "\n",
    "\n",
    "df = pd.DataFrame(res)\n",
    "# df['measure'] = measure\n",
    "# df['measure'][df.insight == 'cross measure correlation'] = ';'.join('measures')\n",
    "# df['measure'][df.insight == 'clustering'] = ';'.join(measures)\n",
    "end_time = time.time()\n",
    "print(\"Time Elapsed: {}\".format(end_time-start_time ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "                Producer Type Energy Source  Year State breakdown  \\\n0                           *             *     *    MS      Year   \n1            Industrial Cogen         Other     *     *      Year   \n2                           *             *     *    IA      Year   \n3                           *             *     *    UT      Year   \n4                           *             *     *    CO      Year   \n..                        ...           ...   ...   ...       ...   \n695          Electric Utility          Coal     *     *      Year   \n696          Electric Utility             *     *     *     State   \n697      Industrial Non-Cogen             *     *     *      Year   \n698                         *             *  2014     *     State   \n699  Utility Sector Non-Cogen     Petroleum     *     *      Year   \n\n    breakdown_value          measure       sig    impact  \\\n0                -1         CO2 (kt)  0.561615  0.014724   \n1                -1         CO2 (kt)  0.300003  0.029060   \n2                -1         CO2 (kt)  0.494322  0.020945   \n3                -1         CO2 (kt)  0.565674  0.015551   \n4                -1         CO2 (kt)  0.521803  0.017251   \n..              ...              ...       ...       ...   \n695              -1  CO2 (kt);SO2 kt  0.650993  0.051414   \n696              -1  CO2 (kt);SO2 kt  0.680802  0.211148   \n697              -1  CO2 (kt);SO2 kt  0.560324  0.057732   \n698              -1  CO2 (kt);SO2 kt  0.724247  0.043590   \n699              -1  CO2 (kt);SO2 kt  0.849840  0.034163   \n\n                       insight insight_type     score  \n0                        trend        shape  0.008269  \n1                        trend        shape  0.008718  \n2                        trend        shape  0.010353  \n3                        trend        shape  0.008797  \n4                        trend        shape  0.009002  \n..                         ...          ...       ...  \n695  cross measure correlation     compound  0.033470  \n696  cross measure correlation     compound  0.143750  \n697  cross measure correlation     compound  0.032348  \n698  cross measure correlation     compound  0.031570  \n699  cross measure correlation     compound  0.029033  \n\n[700 rows x 12 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Producer Type</th>\n      <th>Energy Source</th>\n      <th>Year</th>\n      <th>State</th>\n      <th>breakdown</th>\n      <th>breakdown_value</th>\n      <th>measure</th>\n      <th>sig</th>\n      <th>impact</th>\n      <th>insight</th>\n      <th>insight_type</th>\n      <th>score</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>*</td>\n      <td>*</td>\n      <td>*</td>\n      <td>MS</td>\n      <td>Year</td>\n      <td>-1</td>\n      <td>CO2 (kt)</td>\n      <td>0.561615</td>\n      <td>0.014724</td>\n      <td>trend</td>\n      <td>shape</td>\n      <td>0.008269</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Industrial Cogen</td>\n      <td>Other</td>\n      <td>*</td>\n      <td>*</td>\n      <td>Year</td>\n      <td>-1</td>\n      <td>CO2 (kt)</td>\n      <td>0.300003</td>\n      <td>0.029060</td>\n      <td>trend</td>\n      <td>shape</td>\n      <td>0.008718</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>*</td>\n      <td>*</td>\n      <td>*</td>\n      <td>IA</td>\n      <td>Year</td>\n      <td>-1</td>\n      <td>CO2 (kt)</td>\n      <td>0.494322</td>\n      <td>0.020945</td>\n      <td>trend</td>\n      <td>shape</td>\n      <td>0.010353</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>*</td>\n      <td>*</td>\n      <td>*</td>\n      <td>UT</td>\n      <td>Year</td>\n      <td>-1</td>\n      <td>CO2 (kt)</td>\n      <td>0.565674</td>\n      <td>0.015551</td>\n      <td>trend</td>\n      <td>shape</td>\n      <td>0.008797</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>*</td>\n      <td>*</td>\n      <td>*</td>\n      <td>CO</td>\n      <td>Year</td>\n      <td>-1</td>\n      <td>CO2 (kt)</td>\n      <td>0.521803</td>\n      <td>0.017251</td>\n      <td>trend</td>\n      <td>shape</td>\n      <td>0.009002</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>695</th>\n      <td>Electric Utility</td>\n      <td>Coal</td>\n      <td>*</td>\n      <td>*</td>\n      <td>Year</td>\n      <td>-1</td>\n      <td>CO2 (kt);SO2 kt</td>\n      <td>0.650993</td>\n      <td>0.051414</td>\n      <td>cross measure correlation</td>\n      <td>compound</td>\n      <td>0.033470</td>\n    </tr>\n    <tr>\n      <th>696</th>\n      <td>Electric Utility</td>\n      <td>*</td>\n      <td>*</td>\n      <td>*</td>\n      <td>State</td>\n      <td>-1</td>\n      <td>CO2 (kt);SO2 kt</td>\n      <td>0.680802</td>\n      <td>0.211148</td>\n      <td>cross measure correlation</td>\n      <td>compound</td>\n      <td>0.143750</td>\n    </tr>\n    <tr>\n      <th>697</th>\n      <td>Industrial Non-Cogen</td>\n      <td>*</td>\n      <td>*</td>\n      <td>*</td>\n      <td>Year</td>\n      <td>-1</td>\n      <td>CO2 (kt);SO2 kt</td>\n      <td>0.560324</td>\n      <td>0.057732</td>\n      <td>cross measure correlation</td>\n      <td>compound</td>\n      <td>0.032348</td>\n    </tr>\n    <tr>\n      <th>698</th>\n      <td>*</td>\n      <td>*</td>\n      <td>2014</td>\n      <td>*</td>\n      <td>State</td>\n      <td>-1</td>\n      <td>CO2 (kt);SO2 kt</td>\n      <td>0.724247</td>\n      <td>0.043590</td>\n      <td>cross measure correlation</td>\n      <td>compound</td>\n      <td>0.031570</td>\n    </tr>\n    <tr>\n      <th>699</th>\n      <td>Utility Sector Non-Cogen</td>\n      <td>Petroleum</td>\n      <td>*</td>\n      <td>*</td>\n      <td>Year</td>\n      <td>-1</td>\n      <td>CO2 (kt);SO2 kt</td>\n      <td>0.849840</td>\n      <td>0.034163</td>\n      <td>cross measure correlation</td>\n      <td>compound</td>\n      <td>0.029033</td>\n    </tr>\n  </tbody>\n</table>\n<p>700 rows Ã— 12 columns</p>\n</div>"
     },
     "execution_count": 334,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time Elapsed: 189.3702461719513\n"
     ]
    }
   ],
   "source": [
    "print(\"Time Elapsed: {}\".format(end_time-start_time ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "'\\nprint point insight result\\n'"
     },
     "execution_count": 336,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "print point insight result\n",
    "\"\"\"\n",
    "# for insight in res:\n",
    "#     subspace_cond = {k:insight[k] for k in insight.keys() if k in feature_names}\n",
    "#     condition = dict(subspace_cond)\n",
    "#     selected_df = get_subspace_df(condition, data)\n",
    "#     grouped_df = selected_df.groupby(insight['breakdown'], as_index=False).sum()\n",
    "#     if insight['insight'] == 'top1':\n",
    "#         grouped_df = grouped_df.sort_values(by=[measure])\n",
    "#         grouped_df.plot(x=insight['breakdown'], y=measure, kind='bar')\n",
    "#     elif insight['insight'] == 'trend' or insight['insight'] == 'evenness':\n",
    "#         grouped_df = grouped_df.sort_values(by=time_col)\n",
    "#         grouped_df.plot(x=insight['breakdown'], y=measure, kind='line')\n",
    "#     elif insight['insight'] == 'change point' or insight['insight'] == 'outlier':\n",
    "#         grouped_df = grouped_df.sort_values(by=time_col)\n",
    "#         y_val = grouped_df.loc[grouped_df[insight['breakdown']] == insight['breakdown_value']][measure]\n",
    "#         plt.plot(grouped_df[insight['breakdown']], grouped_df[measure])\n",
    "#         plt.plot([insight['breakdown_value']], [y_val.values[0]],color='red', marker='o', markersize=3)\n",
    "#     elif insight['insight'] == 'attribution':\n",
    "#         grouped_df = grouped_df.sort_values(by=[measure])\n",
    "#         plt.pie(grouped_df[measure], labels = grouped_df[insight['breakdown']], startangle = 90,\n",
    "#         counterclock = False)\n",
    "#         plt.axis('square')\n",
    "#     elif insight['insight'] == 'correlation':\n",
    "#         grouped_df = selected_df.loc[selected_df[insight['breakdown']] == insight['breakdown_value'].split(';')[0]]\n",
    "#         corr_grouped_df = selected_df.loc[selected_df[insight['breakdown']] == insight['breakdown_value'].split(';')[1]]\n",
    "#         grouped_df = grouped_df.groupby(time_col, as_index=False).agg(\n",
    "#                 {time_col: 'first', measure: 'sum'})\n",
    "#         corr_grouped_df = corr_grouped_df.groupby(time_col, as_index=False).agg(\n",
    "#                 {time_col: 'first', measure: 'sum'})\n",
    "#         y1 = grouped_df[measure].values\n",
    "#         y2 = corr_grouped_df[measure].values\n",
    "#         print(grouped_df)\n",
    "#         print(corr_grouped_df)\n",
    "#         print(y1)\n",
    "#         print(y2)\n",
    "#         sig, _ = pearsonr(y1, y2)\n",
    "#         print(\"Sig of correlation: \", sig)\n",
    "#         plt.plot(grouped_df[time_col], grouped_df[measure], color='red')\n",
    "#         plt.plot(corr_grouped_df[time_col], corr_grouped_df[measure], color='blue')\n",
    "#     elif insight['insight'] == 'clustering':\n",
    "#         measure_val_x = np.array(grouped_df[measures[0]].values)\n",
    "#         measure_val_y = np.array(grouped_df[measures[1]].values)\n",
    "#         X = np.vstack((measure_val_x, measure_val_y)).T\n",
    "#         X_scale = StandardScaler().fit_transform(X)\n",
    "#\n",
    "#         # Compute DBSCAN\n",
    "#         db = DBSCAN(eps=0.5, min_samples=5).fit(X_scale)\n",
    "#         core_samples_mask = np.zeros_like(db.labels_, dtype=bool)\n",
    "#         core_samples_mask[db.core_sample_indices_] = True\n",
    "#         labels = db.labels_\n",
    "#         # Number of clusters in labels, ignoring noise if present.\n",
    "#         n_clusters_ = len(set(labels)) - (1 if -1 in labels else 0)\n",
    "#         n_noise_ = list(labels).count(-1)\n",
    "#         counters = collections.Counter(labels)\n",
    "#         max_group = max(counters, key=counters.get)\n",
    "#         max_total_ratio = counters.get(max_group) * 1.0 / len(labels)\n",
    "#         plt.figure(1)\n",
    "#         plt.clf()\n",
    "#\n",
    "#         colors = cycle('bgrcmykbgrcmykbgrcmykbgrcmyk')\n",
    "#         for k, col in zip(range(n_clusters_), colors):\n",
    "#             my_members = labels == k\n",
    "#             plt.plot(X[my_members, 0], X[my_members, 1], col + '.')\n",
    "#             # plt.plot(cluster_center[0], cluster_center[1], 'o', markerfacecolor=col,\n",
    "#             #          markeredgecolor='k', markersize=14)\n",
    "#     elif insight['insight'] == 'cross measure correlation':\n",
    "#         measure_val_x = np.array(grouped_df[measures[0]].values).reshape(-1,1)\n",
    "#         measure_val_y = np.array(grouped_df[measures[1]].values).reshape(-1,1)\n",
    "#         reg = LinearRegression().fit(measure_val_x, measure_val_y)\n",
    "#         plt.scatter(measure_val_x, measure_val_y)\n",
    "#         measure_y_pred = reg.predict(measure_val_x)\n",
    "#         plt.plot(measure_val_x, measure_y_pred)\n",
    "#     plt.title(condition)\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "outputs": [],
   "source": [
    "# df = pd.read_csv('C:\\\\Users\\\\52497\\\\Dropbox\\\\VIS\\\\Insight\\\\res\\\\insights_COVID-19.csv')\n",
    "# for index, row in df.iterrows():\n",
    "#     if row['breakdown'] == time_col and row['breakdown_value'] != '-1':\n",
    "#         df.at[index, 'breakdown_value'] = str(pd.to_datetime(row['breakdown_value']).strftime(\"%Y-%m-%d\"))\n",
    "#     if row['date'] != '*':\n",
    "#         df.at[index, 'date'] = str(pd.to_datetime(row['date']).strftime(\"%Y-%m-%d\"))\n",
    "#\n",
    "with open(insight_data_path, 'w+') as file:\n",
    "    df.to_csv(file, index=False, line_terminator='\\n')\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "pycharm-1cafc576",
   "language": "python",
   "display_name": "PyCharm (Insight)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}